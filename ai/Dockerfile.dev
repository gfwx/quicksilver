# Development Dockerfile for FastAPI with hot-reload - optimized
FROM python:3.11-slim

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .

# Create requirements without torch to prevent reinstallation
RUN grep -v "^torch==" requirements.txt > requirements-no-torch.txt

# Install PyTorch using extra-index-url to prioritize CPU builds
RUN pip install --no-cache-dir \
    torch==2.8.0 \
    --extra-index-url https://download.pytorch.org/whl/cpu

# Install other dependencies (torch already excluded)
RUN pip install --no-cache-dir -r requirements-no-torch.txt

# Set HuggingFace cache location
ENV HF_HOME=/app/.cache/huggingface

# Download sentence transformer model at build time
RUN mkdir -p /app/.cache/huggingface && \
    python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"

# Create data directory
RUN mkdir -p /app/data

# Expose port
EXPOSE 8000

# Note: Source code is mounted via volume in docker-compose.dev.yml
# Command is overridden in docker-compose.dev.yml with --reload flag
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
